import tkinter as tk 
from googlesearch import search
from tkinter import messagebox
import webbrowser
from bs4 import BeautifulSoup
import requests
from PIL import Image, ImageTk
import time
import xlsxwriter
import pandas as pd
from tkinter import messagebox as mb 

from tkinter import filedialog 
import os
websites = []
global x
global excel_df
excel_df = pd.DataFrame(columns=['Anahtar Kelime Ä°Ã§erme Durumu','Åžehir','BaÅŸlÄ±k','Link'])
keywords = []
comparison_list_adana = []
comparison_list_adana2 = []
comparison_list_batman = []
comparison_list_elazÄ±ÄŸ = []
comparison_list_gaziantep = []
comparison_list_hakkari = []
comparison_list_mardin = []
comparison_list_muÄŸla = []
comparison_list_muÅŸ = []
comparison_list_ÅŸanlÄ±urfa = []
comparison_list_ÅŸÄ±rnak = []
comparison_list_tunceli = []
comparison_list_van = []


def get():
	global res
	final_keywords = []
	guess = Entry.get()
	keywords.append(guess)
	global label3
	label3=tk.Label(lower_frame,text="Ä°stediÄŸiniz iletilmiÅŸtir",bg="#ECECF2")
	root.after(4000,label3.destroy)
	label3.pack()
	Entry.delete(0,tk.END)
	N=""
	res = [ele for ele in keywords if ele != N]
	print(res)
	return res

def network_control():
	try:
		requests.get('https://www.google.com/').status_code
		return True
	except:
		messagebox.showerror("UYARI","internet BaÄŸlantÄ±nÄ±zÄ± kontrol edip uygulamaya tekrardan Girin")
		exit()
def adana_check(): 
    res = mb.askquestion('Adana Arama',  
                         'Adana ValiliÄŸinde Aramaya Devam Edilsin mi ?') 
      
    if res == 'yes' :
    	root.after(1000,lambda:adana_web_scraping())   
    else : 
        mb.showinfo('Return', 'Returning to main application')
def adana_check_2(): 
    res = mb.askquestion('Adana Arama',  
                         'Adana ValiliÄŸinde Aramaya Devam Edilsin mi ?') 
      
    if res == 'yes' :
    	root.after(1000,lambda:adana_press_scraping())   
    else : 
        mb.showinfo('Return', 'Returning to main application')
def adana_web_scraping():
	comparison_string= ["",""]
	comparison_list=[]
	a = []
	hrefs=[]
	arr= []
	page_arr = []
	urls= []
	final_urls = []
	word = get()
	url = "http://adana.gov.tr/duyurular"
	page = requests.get(url)
	data = page.text
	soup = BeautifulSoup(data, 'html.parser')
	text = soup.find_all('a',class_='announce-text')
	for line in soup.find_all('a',class_='announce-text'):
		a.append(line.get_text())
	for line in soup.find_all('a',class_='announce-text'):
		hrefs.append(line.get('href'))
	word=[y.lower() for y in word]
	k = [x.replace(' ', '') for x in a]
	k2= k = [x.replace('\r', '') for x in k]
	k3 = k = [x.replace('\n', '') for x in k2]
	k3=[x.lower() for x in k3]
	hrefs = list(dict.fromkeys(hrefs))

	for i in word:
		all_titles = [p for p in k3 if i in p]
		arr.append(all_titles)
	for i in comparison_string:
		all_titles = [p for p in k3 if i in p]
		comparison_list.append(all_titles)
	arr=tuple(filter(lambda x:x!=[], arr))
	mytuplelist = [tuple(item) for item in arr]
	mylist = list(set(mytuplelist))
	comparison_list_new = [elem for elem in arr if elem not in comparison_list ]
	# index=k3.index(comparison_list_new[0][0])
	num1= int(len(comparison_list_new))
	num2 = int(len(comparison_list_adana))
	adana_count = 0
	
	if num1 > num2:
		for i in range(len(comparison_list_new)):
			comparison_list_adana.append(comparison_list_new[i])
			index=k3.index(comparison_list_new[i][0])
			length=len(excel_df)
			urls.append(index)	
		url=hrefs[index]
		for i in urls:
			final_urls.append(hrefs[i])
		page = requests.get("http:"+url)
		data = page.text
		soup_text = BeautifulSoup(data, 'html.parser')
		for line in soup_text.find_all('div',class_='icerik'):
			page_arr.append(line.get_text())
		page_arr = [x.replace('\xa0', '') for x in page_arr]
		page_arr = [x.replace('\r', '') for x in page_arr]
		page_arr = [x.replace('\n', '') for x in page_arr]
		page_arr = [x.replace(' ', '') for x in page_arr]
		page_arr = [x.replace('/', '') for x in page_arr]
		page_arr=[x.lower() for x in page_arr]
		for i in page_arr:
			page_substring = [p for p in page_arr if ("2911" or "5944") in p]
			print(page_substring)
		if len(page_substring) != 0:
			messagebox.showinfo("Adana","Yeni Duyuru YayÄ±nlandÄ± !!!!")
			label_pos_adana = tk.Label(lower_frame,text="Adana ValiliÄŸinde yeni bildiri yayÄ±nlandÄ±")
			label_pos_adana.pack()
			root.after(60000,label_pos_adana.destroy)
			city = "Adana"
			if(len(comparison_list_new[0]) >= 2)and len(comparison_list_new) ==1:
				for i in range(len(comparison_list_new[0])):
					index=k3.index(comparison_list_new[0][i])
					length=len(excel_df)
					urls.append(index)
				url=hrefs[index]
				for i in urls:
					final_urls.append(hrefs[i])
				final_urls = list(dict.fromkeys(final_urls))
				print(final_urls)
				for i in range(len(comparison_list_new[0])):
					length=len(excel_df)	
					excel_df.loc[length+1] = ["+",city,comparison_list_new[0][i],final_urls[i]]
					print(excel_df)
			else:
				length=len(excel_df)
				excel_df.loc[length+1] = ["+",city,comparison_list_new[-1:][0],url]
			
			print(num1," ",num2)
			num2+=1
			print(num1," ",num2)
			adana_check()
		else:
			messagebox.showinfo("Adana","Yeni Duyuru YayÄ±nlandÄ± !!!!")
			label_pos_adana = tk.Label(lower_frame,text="Adana ValiliÄŸinde yeni bildiri yayÄ±nlandÄ±")
			city = "Adana"
			if(len(comparison_list_new[0]) >= 2)and len(comparison_list_new) == 1:
				for i in range(len(comparison_list_new[0])):
					for i in range(len(comparison_list_new[0])):
						index=k3.index(comparison_list_new[0][i])
						length=len(excel_df)
						urls.append(index)
					url=hrefs[index]
					for i in urls:
						final_urls.append(hrefs[i])
					final_urls = list(dict.fromkeys(final_urls))
				for i in range(len(comparison_list_new[0])):
					length=len(excel_df)	
					excel_df.loc[length+1] = ["-",city,comparison_list_new[0][i],final_urls[i]]
					print(excel_df)
			else:
				length=len(excel_df)
				excel_df.loc[length+1] = ["x",city,comparison_list_new[-1:][0],url]
			adana_check()
	else:
		label_negative_adana = tk.Label(lower_frame,text="Adana valiliÄŸinde 5 dakika sonra tekrardan arama yapÄ±lacak")
		label_negative_adana.pack()
		root.after(60000,label_negative_adana.destroy)
		root.after(60000,lambda:adana_web_scraping())
def adana_press_scraping():
	comparison_string= ["",""]
	comparison_list=[]
	a = []
	hrefs=[]
	arr= []
	page_arr = []
	word =get()
	url = "http://adana.gov.tr/basin-aciklamalari"
	page = requests.get(url)
	data = page.text
	soup = BeautifulSoup(data, 'html.parser')
	text = soup.find_all('a',class_='announce-text')
	for line in soup.find_all('a',class_='announce-text'):
		a.append(line.get_text())
	for line in soup.find_all('a',class_='announce-text'):
		hrefs.append(line.get('href'))
	word=[y.lower() for y in word]
	k = [x.replace(' ', '') for x in a]
	k2= k = [x.replace('\r', '') for x in k]
	k3 = k = [x.replace('\n', '') for x in k2]
	k3=[x.lower() for x in k3]
	hrefs = list(dict.fromkeys(hrefs))
	for i in word:
		all_titles = [p for p in k3 if i in p]
		arr.append(all_titles)
	for i in comparison_string:
		all_titles = [p for p in k3 if i in p]
		comparison_list.append(all_titles)
	arr=tuple(filter(lambda x:x!=[], arr))
	mytuplelist = [tuple(item) for item in arr]
	mylist = list(set(mytuplelist))
	comparison_list_new = [elem for elem in arr if elem not in comparison_list ]
	# index=k3.index(comparison_list_new[0][0])
	num1= int(len(comparison_list_new))
	num2 = int(len(comparison_list_adana2))
	stringg = comparison_list_new[-1:]
	if num1 > num2:
		for i in range(len(comparison_list_new)):
			comparison_list_adana2.append(comparison_list_new[i])
			index=k3.index(comparison_list_new[i][0])
		length=len(excel_df)
		url=hrefs[index]
		page = requests.get("http:"+url)
		data = page.text
		soup_text = BeautifulSoup(data, 'html.parser')
		for line in soup_text.find_all('div',class_='icerik'):
			page_arr.append(line.get_text())
		page_arr =  [x.replace('\xa0', '') for x in page_arr]
		page_arr =  [x.replace('\r', '') for x in page_arr]
		page_arr =  [x.replace('\n', '') for x in page_arr]
		page_arr =  [x.replace(' ', '') for x in page_arr]
		page_arr =  [x.replace('/', '') for x in page_arr]
		page_arr =  [x.lower() for x in page_arr]
		for i in page_arr:
			page_substring = [p for p in page_arr if ("2911" or "5944") in p]
			print(page_substring)
		if len(page_substring) != 0:	
			messagebox.showinfo("Adana","Yeni Duyuru YayÄ±nlandÄ± !!!!")
			label_pos_adana = tk.Label(lower_frame,text="Adana ValiliÄŸinde yeni bildiri yayÄ±nlandÄ±")
			label_pos_adana.pack()
			root.after(60000,label_pos_adana.destroy)
			city = "Adana"
			excel_df.loc[length+1] = ["ðŸ—¸",city,comparison_list_new[-1:][0],url]
			print(num1," ",num2)
			num2+=1
			print(num1," ",num2)
			adana_check_2()
		else:
			messagebox.showinfo("Adana","Yeni Duyuru YayÄ±nlandÄ± !!!!")
			label_pos_adana = tk.Label(lower_frame,text="Adana ValiliÄŸinde yeni bildiri yayÄ±nlandÄ±")
			city = "Adana"
			excel_df.loc[length+1] = ["X",city,comparison_list_new[-1:][0],url]
			adana_check_2()
	
		

	else:
		label_negative_adana = tk.Label(lower_frame,text="Adana valiliÄŸinde 5 dakika sonra tekrardan arama yapÄ±lacak")
		label_negative_adana.pack()
		root.after(60000,label_negative_adana.destroy)
		root.after(60000,lambda:adana_press_scraping())


def batman_check(): 
    res = mb.askquestion('batman Arama',  
                         'batman ValiliÄŸinde Aramaya Devam Edilsin mi ?') 
      
    if res == 'yes' :
    	root.after(1000,lambda:batman_web_scraping())   
    else : 
        mb.showinfo('Return', 'Returning to main application')

def batman_web_scraping():
	comparison_string= ["",""]
	comparison_list=[]
	a = []
	hrefs=[]
	arr= []
	page_arr = []
	urls= []
	final_urls = []
	word = get()
	url = "http://batman.gov.tr/duyurular"
	page = requests.get(url)
	data = page.text
	soup = BeautifulSoup(data, 'html.parser')
	text = soup.find_all('a',class_='announce-text')
	for line in soup.find_all('a',class_='announce-text'):
		a.append(line.get_text())
	for line in soup.find_all('a',class_='announce-text'):
		hrefs.append(line.get('href'))
	word=[y.lower() for y in word]
	k = [x.replace(' ', '') for x in a]
	k2= k = [x.replace('\r', '') for x in k]
	k3 = k = [x.replace('\n', '') for x in k2]
	k3=[x.lower() for x in k3]
	hrefs = list(dict.fromkeys(hrefs))

	for i in word:
		all_titles = [p for p in k3 if i in p]
		arr.append(all_titles)
	for i in comparison_string:
		all_titles = [p for p in k3 if i in p]
		comparison_list.append(all_titles)
	arr=tuple(filter(lambda x:x!=[], arr))
	mytuplelist = [tuple(item) for item in arr]
	mylist = list(set(mytuplelist))
	comparison_list_new = [elem for elem in arr if elem not in comparison_list ]
	# index=k3.index(comparison_list_new[0][0])
	num1= int(len(comparison_list_new))
	num2 = int(len(comparison_list_batman))
	batman_count = 0
	
	if num1 > num2:
		for i in range(len(comparison_list_new)):
			comparison_list_batman.append(comparison_list_new[i])
			index=k3.index(comparison_list_new[i][0])
			length=len(excel_df)
			urls.append(index)	
		url=hrefs[index]
		for i in urls:
			final_urls.append(hrefs[i])

		page = requests.get("http:"+url)
		data = page.text
		soup_text = BeautifulSoup(data, 'html.parser')
		for line in soup_text.find_all('div',class_='icerik'):
			page_arr.append(line.get_text())
		page_arr = [x.replace('\xa0', '') for x in page_arr]
		page_arr = [x.replace('\r', '') for x in page_arr]
		page_arr = [x.replace('\n', '') for x in page_arr]
		page_arr = [x.replace(' ', '') for x in page_arr]
		page_arr = [x.replace('/', '') for x in page_arr]
		page_arr=[x.lower() for x in page_arr]
		for i in page_arr:
			page_substring = [p for p in page_arr if ("2911" or "5944") in p]
			print(page_substring)
		if len(page_substring) != 0:
			messagebox.showinfo("Batman","Yeni Duyuru YayÄ±nlandÄ± !!!!")
			label_pos_batman = tk.Label(lower_frame,text="Batman ValiliÄŸinde yeni bildiri yayÄ±nlandÄ±")
			label_pos_batman.pack()
			root.after(60000,label_pos_batman.destroy)
			city = "Batman"
			if(len(comparison_list_new[0]) >= 2)and len(comparison_list_new) ==1:
				for i in range(len(comparison_list_new[0])):
					index=k3.index(comparison_list_new[0][i])
					length=len(excel_df)
					urls.append(index)
				url=hrefs[index]
				for i in urls:
					final_urls.append(hrefs[i])
				final_urls = list(dict.fromkeys(final_urls))
				print(final_urls)
				for i in range(len(comparison_list_new[0])):
					length=len(excel_df)	
					excel_df.loc[length+1] = ["+",city,comparison_list_new[0][i],final_urls[i]]
					print(excel_df)
			else:
				length=len(excel_df)
				excel_df.loc[length+1] = ["+",city,comparison_list_new[-1:][0],url]
			
			print(num1," ",num2)
			num2+=1
			print(num1," ",num2)
			batman_check()
		else:
			messagebox.showinfo("Batman","Yeni Duyuru YayÄ±nlandÄ± !!!!")
			label_pos_batman = tk.Label(lower_frame,text="Batman ValiliÄŸinde yeni bildiri yayÄ±nlandÄ±")
			city = "Batman"
			if(len(comparison_list_new[0]) >= 2)and len(comparison_list_new) == 1:
				for i in range(len(comparison_list_new[0])):
					for i in range(len(comparison_list_new[0])):
						index=k3.index(comparison_list_new[0][i])
						length=len(excel_df)
						urls.append(index)
					url=hrefs[index]
					for i in urls:
						final_urls.append(hrefs[i])
					final_urls = list(dict.fromkeys(final_urls))
				for i in range(len(comparison_list_new[0])):
					length=len(excel_df)	
					excel_df.loc[length+1] = ["-",city,comparison_list_new[0][i],final_urls[i]]
					print(excel_df)
			else:
				length=len(excel_df)
				excel_df.loc[length+1] = ["x",city,comparison_list_new[-1:][0],url]
			batman_check()
		

	else:
		label_negative_batman = tk.Label(lower_frame,text="Batman valiliÄŸinde 5 dakika sonra tekrardan arama yapÄ±lacak")
		label_negative_batman.pack()
		root.after(60000,label_negative_batman.destroy)
		root.after(60000,lambda:batman_web_scraping())

def elazÄ±ÄŸ_check(): 
    res = mb.askquestion('ElazÄ±ÄŸ Arama',  
                         'ElazÄ±ÄŸ ValiliÄŸinde Aramaya Devam Edilsin mi ?') 
      
    if res == 'yes' :
    	root.after(1000,lambda:elazÄ±ÄŸ_web_scraping())   
    else : 
        mb.showinfo('Return', 'Returning to main application')
def elazÄ±ÄŸ_web_scraping():
	comparison_string= ["",""]
	comparison_list=[]
	a = []
	hrefs=[]
	arr= []
	page_arr = []
	urls= []
	final_urls = []
	word =get()
	url = "http://www.elazig.gov.tr/duyurular"
	page = requests.get(url)
	data = page.text
	soup = BeautifulSoup(data, 'html.parser')
	text = soup.find_all('a',class_='announce-text')
	for line in soup.find_all('a',class_='announce-text'):
		a.append(line.get_text())
	for line in soup.find_all('a',class_='announce-text'):
		hrefs.append(line.get('href'))
	word=[y.lower() for y in word]
	k = [x.replace(' ', '') for x in a]
	k2= k = [x.replace('\r', '') for x in k]
	k3 = k = [x.replace('\n', '') for x in k2]
	k3=[x.lower() for x in k3]
	hrefs = list(dict.fromkeys(hrefs))

	for i in word:
		all_titles = [p for p in k3 if i in p]
		arr.append(all_titles)
	for i in comparison_string:
		all_titles = [p for p in k3 if i in p]
		comparison_list.append(all_titles)
	arr=tuple(filter(lambda x:x!=[], arr))
	mytuplelist = [tuple(item) for item in arr]
	mylist = list(set(mytuplelist))
	comparison_list_new = [elem for elem in arr if elem not in comparison_list ]
	# index=k3.index(comparison_list_new[0][0])
	num1= int(len(comparison_list_new))
	num2 = int(len(comparison_list_elazÄ±ÄŸ))
	elazÄ±ÄŸ_count = 0
	
	if num1 > num2:
		for i in range(len(comparison_list_new)):
			comparison_list_elazÄ±ÄŸ.append(comparison_list_new[i])
			index=k3.index(comparison_list_new[i][0])
			length=len(excel_df)
			urls.append(index)	
		url=hrefs[index]
		for i in urls:
			final_urls.append(hrefs[i])

		page = requests.get("http:"+url)
		data = page.text
		soup_text = BeautifulSoup(data, 'html.parser')
		for line in soup_text.find_all('div',class_='icerik'):
			page_arr.append(line.get_text())
		page_arr = [x.replace('\xa0', '') for x in page_arr]
		page_arr = [x.replace('\r', '') for x in page_arr]
		page_arr = [x.replace('\n', '') for x in page_arr]
		page_arr = [x.replace(' ', '') for x in page_arr]
		page_arr = [x.replace('/', '') for x in page_arr]
		page_arr=[x.lower() for x in page_arr]
		for i in page_arr:
			page_substring = [p for p in page_arr if ("2911" or "5944") in p]
			print(page_substring)
		if len(page_substring) != 0:
			messagebox.showinfo("ElazÄ±ÄŸ","Yeni Duyuru YayÄ±nlandÄ± !!!!")
			label_pos_elazÄ±ÄŸ = tk.Label(lower_frame,text="ElazÄ±ÄŸ ValiliÄŸinde yeni bildiri yayÄ±nlandÄ±")
			label_pos_elazÄ±ÄŸ.pack()
			root.after(60000,label_pos_elazÄ±ÄŸ.destroy)
			city = "ElazÄ±ÄŸ"
			if(len(comparison_list_new[0]) >= 2)and len(comparison_list_new) ==1:
				for i in range(len(comparison_list_new[0])):
					index=k3.index(comparison_list_new[0][i])
					length=len(excel_df)
					urls.append(index)
				url=hrefs[index]
				for i in urls:
					final_urls.append(hrefs[i])
				final_urls = list(dict.fromkeys(final_urls))
				print(final_urls)
				for i in range(len(comparison_list_new[0])):
					length=len(excel_df)	
					excel_df.loc[length+1] = ["+",city,comparison_list_new[0][i],final_urls[i]]
					print(excel_df)
			else:
				length=len(excel_df)
				excel_df.loc[length+1] = ["+",city,comparison_list_new[-1:][0],url]
			
			print(num1," ",num2)
			num2+=1
			print(num1," ",num2)
			elazÄ±ÄŸ_check()
		else:
			messagebox.showinfo("ElazÄ±ÄŸ","Yeni Duyuru YayÄ±nlandÄ± !!!!")
			label_pos_elazÄ±ÄŸ = tk.Label(lower_frame,text="ElazÄ±ÄŸ ValiliÄŸinde yeni bildiri yayÄ±nlandÄ±")
			city = "ElazÄ±ÄŸ"
			if(len(comparison_list_new[0]) >= 2)and len(comparison_list_new) == 1:
				for i in range(len(comparison_list_new[0])):
					for i in range(len(comparison_list_new[0])):
						index=k3.index(comparison_list_new[0][i])
						length=len(excel_df)
						urls.append(index)
					url=hrefs[index]
					for i in urls:
						final_urls.append(hrefs[i])
					final_urls = list(dict.fromkeys(final_urls))
				for i in range(len(comparison_list_new[0])):
					length=len(excel_df)	
					excel_df.loc[length+1] = ["-",city,comparison_list_new[0][i],final_urls[i]]
					print(excel_df)
			else:
				length=len(excel_df)
				excel_df.loc[length+1] = ["x",city,comparison_list_new[-1:][0],url]
			elazÄ±ÄŸ_check()
		

	else:
		label_negative_elazÄ±ÄŸ = tk.Label(lower_frame,text="ElazÄ±ÄŸ valiliÄŸinde 5 dakika sonra tekrardan arama yapÄ±lacak")
		label_negative_elazÄ±ÄŸ.pack()
		root.after(60000,label_negative_elazÄ±ÄŸ.destroy)
		root.after(60000,lambda:elazÄ±ÄŸ_web_scraping())
def gaziantep_check(): 
    res = mb.askquestion('Gaziantep Arama',  
                         'Gaziantep ValiliÄŸinde Aramaya Devam Edilsin mi ?') 
      
    if res == 'yes' :
    	root.after(1000,lambda:gaziantep_web_scraping())   
    else : 
        mb.showinfo('Return', 'Returning to main application')
def gaziantep_web_scraping():
	comparison_string= ["",""]
	comparison_list=[]
	a = []
	hrefs=[]
	arr= []
	page_arr = []
	urls= []
	final_urls = []
	word =get()
	url = "http://gaziantep.gov.tr/duyurular"
	page = requests.get(url)
	data = page.text
	soup = BeautifulSoup(data, 'html.parser')
	text = soup.find_all('a',class_='announce-text')
	for line in soup.find_all('a',class_='announce-text'):
		a.append(line.get_text())
	for line in soup.find_all('a',class_='announce-text'):
		hrefs.append(line.get('href'))
	word=[y.lower() for y in word]
	k = [x.replace(' ', '') for x in a]
	k2= k = [x.replace('\r', '') for x in k]
	k3 = k = [x.replace('\n', '') for x in k2]
	k3=[x.lower() for x in k3]
	hrefs = list(dict.fromkeys(hrefs))

	for i in word:
		all_titles = [p for p in k3 if i in p]
		arr.append(all_titles)
	for i in comparison_string:
		all_titles = [p for p in k3 if i in p]
		comparison_list.append(all_titles)
	arr=tuple(filter(lambda x:x!=[], arr))
	mytuplelist = [tuple(item) for item in arr]
	mylist = list(set(mytuplelist))
	comparison_list_new = [elem for elem in arr if elem not in comparison_list ]
	# index=k3.index(comparison_list_new[0][0])
	num1= int(len(comparison_list_new))
	num2 = int(len(comparison_list_gaziantep))
	gaziantep_count = 0
	
	if num1 > num2:
		for i in range(len(comparison_list_new)):
			comparison_list_gaziantep.append(comparison_list_new[i])
			index=k3.index(comparison_list_new[i][0])
			length=len(excel_df)
			urls.append(index)	
		url=hrefs[index]
		for i in urls:
			final_urls.append(hrefs[i])

		page = requests.get("http:"+url)
		data = page.text
		soup_text = BeautifulSoup(data, 'html.parser')
		for line in soup_text.find_all('div',class_='icerik'):
			page_arr.append(line.get_text())
		page_arr = [x.replace('\xa0', '') for x in page_arr]
		page_arr = [x.replace('\r', '') for x in page_arr]
		page_arr = [x.replace('\n', '') for x in page_arr]
		page_arr = [x.replace(' ', '') for x in page_arr]
		page_arr = [x.replace('/', '') for x in page_arr]
		page_arr=[x.lower() for x in page_arr]
		for i in page_arr:
			page_substring = [p for p in page_arr if ("2911" or "5944") in p]
			print(page_substring)
		if len(page_substring) != 0:
			messagebox.showinfo("Gaziantep","Yeni Duyuru YayÄ±nlandÄ± !!!!")
			label_pos_gaziantep = tk.Label(lower_frame,text="Gaziantep ValiliÄŸinde yeni bildiri yayÄ±nlandÄ±")
			label_pos_gaziantep.pack()
			root.after(60000,label_pos_gaziantep.destroy)
			city = "Gaziantep"
			if(len(comparison_list_new[0]) >= 2)and len(comparison_list_new) ==1:
				for i in range(len(comparison_list_new[0])):
					index=k3.index(comparison_list_new[0][i])
					length=len(excel_df)
					urls.append(index)
				url=hrefs[index]
				for i in urls:
					final_urls.append(hrefs[i])
				final_urls = list(dict.fromkeys(final_urls))
				print(final_urls)
				for i in range(len(comparison_list_new[0])):
					length=len(excel_df)	
					excel_df.loc[length+1] = ["+",city,comparison_list_new[0][i],final_urls[i]]
					print(excel_df)
			else:
				length=len(excel_df)
				excel_df.loc[length+1] = ["+",city,comparison_list_new[-1:][0],url]
			
			print(num1," ",num2)
			num2+=1
			print(num1," ",num2)
			gaziantep_check()
		else:
			messagebox.showinfo("Gaziantep","Yeni Duyuru YayÄ±nlandÄ± !!!!")
			label_pos_gaziantep = tk.Label(lower_frame,text="Gaziantep ValiliÄŸinde yeni bildiri yayÄ±nlandÄ±")
			city = "Gaziantep"
			if(len(comparison_list_new[0]) >= 2)and len(comparison_list_new) == 1:
				for i in range(len(comparison_list_new[0])):
					for i in range(len(comparison_list_new[0])):
						index=k3.index(comparison_list_new[0][i])
						length=len(excel_df)
						urls.append(index)
					url=hrefs[index]
					for i in urls:
						final_urls.append(hrefs[i])
					final_urls = list(dict.fromkeys(final_urls))
				for i in range(len(comparison_list_new[0])):
					length=len(excel_df)	
					excel_df.loc[length+1] = ["-",city,comparison_list_new[0][i],final_urls[i]]
					print(excel_df)
			else:
				length=len(excel_df)
				excel_df.loc[length+1] = ["x",city,comparison_list_new[-1:][0],url]
			gaziantep_check()
		

	else:
		label_negative_gaziantep = tk.Label(lower_frame,text="Gaziantep valiliÄŸinde 5 dakika sonra tekrardan arama yapÄ±lacak")
		label_negative_gaziantep.pack()
		root.after(60000,label_negative_gaziantep.destroy)
		root.after(60000,lambda:gaziantep_web_scraping())
def hakkari_check(): 
    res = mb.askquestion('Hakkari Arama',  
                         'Hakkari ValiliÄŸinde Aramaya Devam Edilsin mi ?') 
      
    if res == 'yes' :
    	root.after(1000,lambda:hakkari_web_scraping())   
    else : 
        mb.showinfo('Return', 'Returning to main application')
def hakkari_web_scraping():
	comparison_string= ["",""]
	comparison_list=[]
	a = []
	hrefs=[]
	arr= []
	page_arr = []
	urls= []
	final_urls = []
	word =get()
	url = "http://hakkari.gov.tr/duyurular"
	page = requests.get(url)
	data = page.text
	soup = BeautifulSoup(data, 'html.parser')
	text = soup.find_all('a',class_='announce-text')
	for line in soup.find_all('a',class_='announce-text'):
		a.append(line.get_text())
	for line in soup.find_all('a',class_='announce-text'):
		hrefs.append(line.get('href'))
	word=[y.lower() for y in word]
	k = [x.replace(' ', '') for x in a]
	k2= k = [x.replace('\r', '') for x in k]
	k3 = k = [x.replace('\n', '') for x in k2]
	k3=[x.lower() for x in k3]
	hrefs = list(dict.fromkeys(hrefs))

	for i in word:
		all_titles = [p for p in k3 if i in p]
		arr.append(all_titles)
	for i in comparison_string:
		all_titles = [p for p in k3 if i in p]
		comparison_list.append(all_titles)
	arr=tuple(filter(lambda x:x!=[], arr))
	mytuplelist = [tuple(item) for item in arr]
	mylist = list(set(mytuplelist))
	comparison_list_new = [elem for elem in arr if elem not in comparison_list ]
	# index=k3.index(comparison_list_new[0][0])
	num1= int(len(comparison_list_new))
	num2 = int(len(comparison_list_hakkari))
	hakkari_count = 0
	
	if num1 > num2:
		for i in range(len(comparison_list_new)):
			comparison_list_hakkari.append(comparison_list_new[i])
			index=k3.index(comparison_list_new[i][0])
			length=len(excel_df)
			urls.append(index)	
		url=hrefs[index]
		for i in urls:
			final_urls.append(hrefs[i])

		page = requests.get("http:"+url)
		data = page.text
		soup_text = BeautifulSoup(data, 'html.parser')
		for line in soup_text.find_all('div',class_='icerik'):
			page_arr.append(line.get_text())
		page_arr = [x.replace('\xa0', '') for x in page_arr]
		page_arr = [x.replace('\r', '') for x in page_arr]
		page_arr = [x.replace('\n', '') for x in page_arr]
		page_arr = [x.replace(' ', '') for x in page_arr]
		page_arr = [x.replace('/', '') for x in page_arr]
		page_arr=[x.lower() for x in page_arr]
		for i in page_arr:
			page_substring = [p for p in page_arr if ("2911" or "5944") in p]
			print(page_substring)
		if len(page_substring) != 0:
			messagebox.showinfo("Hakkari","Yeni Duyuru YayÄ±nlandÄ± !!!!")
			label_pos_hakkari = tk.Label(lower_frame,text="Hakkari ValiliÄŸinde yeni bildiri yayÄ±nlandÄ±")
			label_pos_hakkari.pack()
			root.after(60000,label_pos_hakkari.destroy)
			city = "Hakkari"
			if(len(comparison_list_new[0]) >= 2)and len(comparison_list_new) ==1:
				for i in range(len(comparison_list_new[0])):
					index=k3.index(comparison_list_new[0][i])
					length=len(excel_df)
					urls.append(index)
				url=hrefs[index]
				for i in urls:
					final_urls.append(hrefs[i])
				final_urls = list(dict.fromkeys(final_urls))
				print(final_urls)
				for i in range(len(comparison_list_new[0])):
					length=len(excel_df)	
					excel_df.loc[length+1] = ["+",city,comparison_list_new[0][i],final_urls[i]]
					print(excel_df)
			else:
				length=len(excel_df)
				excel_df.loc[length+1] = ["+",city,comparison_list_new[-1:][0],url]
			
			print(num1," ",num2)
			num2+=1
			print(num1," ",num2)
			hakkari_check()
		else:
			messagebox.showinfo("Hakkari","Yeni Duyuru YayÄ±nlandÄ± !!!!")
			label_pos_hakkari = tk.Label(lower_frame,text="Hakkari ValiliÄŸinde yeni bildiri yayÄ±nlandÄ±")
			city = "Hakkari"
			if(len(comparison_list_new[0]) >= 2)and len(comparison_list_new) == 1:
				for i in range(len(comparison_list_new[0])):
					for i in range(len(comparison_list_new[0])):
						index=k3.index(comparison_list_new[0][i])
						length=len(excel_df)
						urls.append(index)
					url=hrefs[index]
					for i in urls:
						final_urls.append(hrefs[i])
					final_urls = list(dict.fromkeys(final_urls))
				for i in range(len(comparison_list_new[0])):
					length=len(excel_df)	
					excel_df.loc[length+1] = ["-",city,comparison_list_new[0][i],final_urls[i]]
					print(excel_df)
			else:
				length=len(excel_df)
				excel_df.loc[length+1] = ["x",city,comparison_list_new[-1:][0],url]
			hakkari_check()
		

	else:
		label_negative_hakkari = tk.Label(lower_frame,text="Hakkari valiliÄŸinde 5 dakika sonra tekrardan arama yapÄ±lacak")
		label_negative_hakkari.pack()
		root.after(60000,label_negative_hakkari.destroy)
		root.after(60000,lambda:hakkari_web_scraping())
def mardin_check(): 
    res = mb.askquestion('Mardin Arama',  
                         'Mardin ValiliÄŸinde Aramaya Devam Edilsin mi ?') 
      
    if res == 'yes' :
    	root.after(1000,lambda:mardin_web_scraping())   
    else : 
        mb.showinfo('Return', 'Returning to the main application')
def mardin_web_scraping():
	comparison_string= ["",""]
	comparison_list=[]
	a = []
	hrefs=[]
	arr= []
	page_arr = []
	urls= []
	final_urls = []
	word =get()
	url = "http://mardin.gov.tr/duyurular"
	page = requests.get(url)
	data = page.text
	soup = BeautifulSoup(data, 'html.parser')
	text = soup.find_all('a',class_='announce-text')
	for line in soup.find_all('a',class_='announce-text'):
		a.append(line.get_text())
	for line in soup.find_all('a',class_='announce-text'):
		hrefs.append(line.get('href'))
	word=[y.lower() for y in word]
	k = [x.replace(' ', '') for x in a]
	k2= k = [x.replace('\r', '') for x in k]
	k3 = k = [x.replace('\n', '') for x in k2]
	k3=[x.lower() for x in k3]
	hrefs = list(dict.fromkeys(hrefs))

	for i in word:
		all_titles = [p for p in k3 if i in p]
		arr.append(all_titles)
	for i in comparison_string:
		all_titles = [p for p in k3 if i in p]
		comparison_list.append(all_titles)
	arr=tuple(filter(lambda x:x!=[], arr))
	mytuplelist = [tuple(item) for item in arr]
	mylist = list(set(mytuplelist))
	comparison_list_new = [elem for elem in arr if elem not in comparison_list ]
	# index=k3.index(comparison_list_new[0][0])
	num1= int(len(comparison_list_new))
	num2 = int(len(comparison_list_mardin))
	mardin_count = 0
	
	if num1 > num2:
		for i in range(len(comparison_list_new)):
			comparison_list_mardin.append(comparison_list_new[i])
			index=k3.index(comparison_list_new[i][0])
			length=len(excel_df)
			urls.append(index)	
		url=hrefs[index]
		for i in urls:
			final_urls.append(hrefs[i])

		page = requests.get("http:"+url)
		data = page.text
		soup_text = BeautifulSoup(data, 'html.parser')
		for line in soup_text.find_all('div',class_='icerik'):
			page_arr.append(line.get_text())
		page_arr = [x.replace('\xa0', '') for x in page_arr]
		page_arr = [x.replace('\r', '') for x in page_arr]
		page_arr = [x.replace('\n', '') for x in page_arr]
		page_arr = [x.replace(' ', '') for x in page_arr]
		page_arr = [x.replace('/', '') for x in page_arr]
		page_arr=[x.lower() for x in page_arr]
		for i in page_arr:
			page_substring = [p for p in page_arr if ("2911" or "5944") in p]
			print(page_substring)
		if len(page_substring) != 0:
			messagebox.showinfo("Mardin","Yeni Duyuru YayÄ±nlandÄ± !!!!")
			label_pos_mardin = tk.Label(lower_frame,text="Mardin ValiliÄŸinde yeni bildiri yayÄ±nlandÄ±")
			label_pos_mardin.pack()
			root.after(60000,label_pos_mardin.destroy)
			city = "Mardin"
			if(len(comparison_list_new[0]) >= 2)and len(comparison_list_new) ==1:
				for i in range(len(comparison_list_new[0])):
					index=k3.index(comparison_list_new[0][i])
					length=len(excel_df)
					urls.append(index)
				url=hrefs[index]
				for i in urls:
					final_urls.append(hrefs[i])
				final_urls = list(dict.fromkeys(final_urls))
				print(final_urls)
				for i in range(len(comparison_list_new[0])):
					length=len(excel_df)	
					excel_df.loc[length+1] = ["+",city,comparison_list_new[0][i],final_urls[i]]
					print(excel_df)
			else:
				length=len(excel_df)
				excel_df.loc[length+1] = ["+",city,comparison_list_new[-1:][0],url]
			
			print(num1," ",num2)
			num2+=1
			print(num1," ",num2)
			mardin_check()
		else:
			messagebox.showinfo("Mardin","Yeni Duyuru YayÄ±nlandÄ± !!!!")
			label_pos_mardin = tk.Label(lower_frame,text="Mardin ValiliÄŸinde yeni bildiri yayÄ±nlandÄ±")
			city = "Mardin"
			if(len(comparison_list_new[0]) >= 2)and len(comparison_list_new) == 1:
				for i in range(len(comparison_list_new[0])):
					for i in range(len(comparison_list_new[0])):
						index=k3.index(comparison_list_new[0][i])
						length=len(excel_df)
						urls.append(index)
					url=hrefs[index]
					for i in urls:
						final_urls.append(hrefs[i])
					final_urls = list(dict.fromkeys(final_urls))
				for i in range(len(comparison_list_new[0])):
					length=len(excel_df)	
					excel_df.loc[length+1] = ["-",city,comparison_list_new[0][i],final_urls[i]]
					print(excel_df)
			else:
				length=len(excel_df)
				excel_df.loc[length+1] = ["x",city,comparison_list_new[-1:][0],url]
			mardin_check()
		

	else:
		label_negative_mardin = tk.Label(lower_frame,text="Mardin valiliÄŸinde 5 dakika sonra tekrardan arama yapÄ±lacak")
		label_negative_mardin.pack()
		root.after(60000,label_negative_mardin.destroy)
		root.after(60000,lambda:mardin_web_scraping())
def muÄŸla_check(): 
    res = mb.askquestion('MuÄŸla Arama',  
                         'MuÄŸla ValiliÄŸinde Aramaya Devam Edilsin mi ?') 
      
    if res == 'yes' :
    	root.after(1000,lambda:muÄŸla_web_scraping())   
    else : 
        mb.showinfo('Return', 'Returning to the main application')
def muÄŸla_web_scraping():
	comparison_string= ["",""]
	comparison_list=[]
	a = []
	hrefs=[]
	arr= []
	page_arr = []
	urls= []
	final_urls = []
	word =get()
	url = "http://mugla.gov.tr/duyurular"
	page = requests.get(url)
	data = page.text
	soup = BeautifulSoup(data, 'html.parser')
	text = soup.find_all('a',class_='announce-text')
	for line in soup.find_all('a',class_='announce-text'):
		a.append(line.get_text())
	for line in soup.find_all('a',class_='announce-text'):
		hrefs.append(line.get('href'))
	word=[y.lower() for y in word]
	k = [x.replace(' ', '') for x in a]
	k2= k = [x.replace('\r', '') for x in k]
	k3 = k = [x.replace('\n', '') for x in k2]
	k3=[x.lower() for x in k3]
	hrefs = list(dict.fromkeys(hrefs))

	for i in word:
		all_titles = [p for p in k3 if i in p]
		arr.append(all_titles)
	for i in comparison_string:
		all_titles = [p for p in k3 if i in p]
		comparison_list.append(all_titles)
	arr=tuple(filter(lambda x:x!=[], arr))
	mytuplelist = [tuple(item) for item in arr]
	mylist = list(set(mytuplelist))
	comparison_list_new = [elem for elem in arr if elem not in comparison_list ]
	# index=k3.index(comparison_list_new[0][0])
	num1= int(len(comparison_list_new))
	num2 = int(len(comparison_list_muÄŸla))
	muÄŸla_count = 0
	
	if num1 > num2:
		for i in range(len(comparison_list_new)):
			comparison_list_muÄŸla.append(comparison_list_new[i])
			index=k3.index(comparison_list_new[i][0])
			length=len(excel_df)
			urls.append(index)	
		url=hrefs[index]
		for i in urls:
			final_urls.append(hrefs[i])

		page = requests.get("http:"+url)
		data = page.text
		soup_text = BeautifulSoup(data, 'html.parser')
		for line in soup_text.find_all('div',class_='icerik'):
			page_arr.append(line.get_text())
		page_arr = [x.replace('\xa0', '') for x in page_arr]
		page_arr = [x.replace('\r', '') for x in page_arr]
		page_arr = [x.replace('\n', '') for x in page_arr]
		page_arr = [x.replace(' ', '') for x in page_arr]
		page_arr = [x.replace('/', '') for x in page_arr]
		page_arr=[x.lower() for x in page_arr]
		for i in page_arr:
			page_substring = [p for p in page_arr if ("2911" or "5944") in p]
			print(page_substring)
		if len(page_substring) != 0:
			messagebox.showinfo("MuÄŸla","Yeni Duyuru YayÄ±nlandÄ± !!!!")
			label_pos_muÄŸla = tk.Label(lower_frame,text="MuÄŸla ValiliÄŸinde yeni bildiri yayÄ±nlandÄ±")
			label_pos_muÄŸla.pack()
			root.after(60000,label_pos_muÄŸla.destroy)
			city = "MuÄŸla"
			if(len(comparison_list_new[0]) >= 2)and len(comparison_list_new) ==1:
				for i in range(len(comparison_list_new[0])):
					index=k3.index(comparison_list_new[0][i])
					length=len(excel_df)
					urls.append(index)
				url=hrefs[index]
				for i in urls:
					final_urls.append(hrefs[i])
				final_urls = list(dict.fromkeys(final_urls))
				print(final_urls)
				for i in range(len(comparison_list_new[0])):
					length=len(excel_df)	
					excel_df.loc[length+1] = ["+",city,comparison_list_new[0][i],final_urls[i]]
					print(excel_df)
			else:
				length=len(excel_df)
				excel_df.loc[length+1] = ["+",city,comparison_list_new[-1:][0],url]
			
			print(num1," ",num2)
			num2+=1
			print(num1," ",num2)
			muÄŸla_check()
		else:
			messagebox.showinfo("MuÄŸla","Yeni Duyuru YayÄ±nlandÄ± !!!!")
			label_pos_muÄŸla = tk.Label(lower_frame,text="MuÄŸla ValiliÄŸinde yeni bildiri yayÄ±nlandÄ±")
			city = "MuÄŸla"
			if(len(comparison_list_new[0]) >= 2)and len(comparison_list_new) == 1:
				for i in range(len(comparison_list_new[0])):
					for i in range(len(comparison_list_new[0])):
						index=k3.index(comparison_list_new[0][i])
						length=len(excel_df)
						urls.append(index)
					url=hrefs[index]
					for i in urls:
						final_urls.append(hrefs[i])
					final_urls = list(dict.fromkeys(final_urls))
				for i in range(len(comparison_list_new[0])):
					length=len(excel_df)	
					excel_df.loc[length+1] = ["-",city,comparison_list_new[0][i],final_urls[i]]
					print(excel_df)
			else:
				length=len(excel_df)
				excel_df.loc[length+1] = ["x",city,comparison_list_new[-1:][0],url]
			muÄŸla_check()
		

	else:
		label_negative_muÄŸla = tk.Label(lower_frame,text="MuÄŸla valiliÄŸinde 5 dakika sonra tekrardan arama yapÄ±lacak")
		label_negative_muÄŸla.pack()
		root.after(60000,label_negative_muÄŸla.destroy)
		root.after(60000,lambda:muÄŸla_web_scraping())

def muÅŸ_check(): 
    res = mb.askquestion('MuÅŸ Arama',  
                         'MuÅŸ ValiliÄŸinde Aramaya Devam Edilsin mi ?') 
      
    if res == 'yes' :
    	root.after(1000,lambda:muÅŸ_web_scraping())   
    else : 
        mb.showinfo('Return', 'Returning to the main application')
def muÅŸ_web_scraping():
	comparison_string= ["",""]
	comparison_list=[]
	a = []
	hrefs=[]
	arr= []
	page_arr = []
	urls= []
	final_urls = []
	word =get()
	url = "http://mus.gov.tr/duyurular"
	page = requests.get(url)
	data = page.text
	soup = BeautifulSoup(data, 'html.parser')
	text = soup.find_all('a',class_='announce-text')
	for line in soup.find_all('a',class_='announce-text'):
		a.append(line.get_text())
	for line in soup.find_all('a',class_='announce-text'):
		hrefs.append(line.get('href'))
	word=[y.lower() for y in word]
	k = [x.replace(' ', '') for x in a]
	k2= k = [x.replace('\r', '') for x in k]
	k3 = k = [x.replace('\n', '') for x in k2]
	k3=[x.lower() for x in k3]
	hrefs = list(dict.fromkeys(hrefs))

	for i in word:
		all_titles = [p for p in k3 if i in p]
		arr.append(all_titles)
	for i in comparison_string:
		all_titles = [p for p in k3 if i in p]
		comparison_list.append(all_titles)
	arr=tuple(filter(lambda x:x!=[], arr))
	mytuplelist = [tuple(item) for item in arr]
	mylist = list(set(mytuplelist))
	comparison_list_new = [elem for elem in arr if elem not in comparison_list ]
	# index=k3.index(comparison_list_new[0][0])
	num1= int(len(comparison_list_new))
	num2 = int(len(comparison_list_muÅŸ))
	muÅŸ_count = 0
	
	if num1 > num2:
		for i in range(len(comparison_list_new)):
			comparison_list_muÅŸ.append(comparison_list_new[i])
			index=k3.index(comparison_list_new[i][0])
			length=len(excel_df)
			urls.append(index)	
		url=hrefs[index]
		for i in urls:
			final_urls.append(hrefs[i])

		page = requests.get("http:"+url)
		data = page.text
		soup_text = BeautifulSoup(data, 'html.parser')
		for line in soup_text.find_all('div',class_='icerik'):
			page_arr.append(line.get_text())
		page_arr = [x.replace('\xa0', '') for x in page_arr]
		page_arr = [x.replace('\r', '') for x in page_arr]
		page_arr = [x.replace('\n', '') for x in page_arr]
		page_arr = [x.replace(' ', '') for x in page_arr]
		page_arr = [x.replace('/', '') for x in page_arr]
		page_arr=[x.lower() for x in page_arr]
		for i in page_arr:
			page_substring = [p for p in page_arr if ("2911" or "5944") in p]
			print(page_substring)
		if len(page_substring) != 0:
			messagebox.showinfo("MuÅŸ","Yeni Duyuru YayÄ±nlandÄ± !!!!")
			label_pos_muÅŸ = tk.Label(lower_frame,text="MuÅŸ ValiliÄŸinde yeni bildiri yayÄ±nlandÄ±")
			label_pos_muÅŸ.pack()
			root.after(60000,label_pos_muÅŸ.destroy)
			city = "MuÅŸ"
			if(len(comparison_list_new[0]) >= 2)and len(comparison_list_new) ==1:
				for i in range(len(comparison_list_new[0])):
					index=k3.index(comparison_list_new[0][i])
					length=len(excel_df)
					urls.append(index)
				url=hrefs[index]
				for i in urls:
					final_urls.append(hrefs[i])
				final_urls = list(dict.fromkeys(final_urls))
				print(final_urls)
				for i in range(len(comparison_list_new[0])):
					length=len(excel_df)	
					excel_df.loc[length+1] = ["+",city,comparison_list_new[0][i],final_urls[i]]
					print(excel_df)
			else:
				length=len(excel_df)
				excel_df.loc[length+1] = ["+",city,comparison_list_new[-1:][0],url]
			
			print(num1," ",num2)
			num2+=1
			print(num1," ",num2)
			muÅŸ_check()
		else:
			messagebox.showinfo("MuÅŸ","Yeni Duyuru YayÄ±nlandÄ± !!!!")
			label_pos_muÅŸ = tk.Label(lower_frame,text="MuÅŸ ValiliÄŸinde yeni bildiri yayÄ±nlandÄ±")
			city = "MuÅŸ"
			if(len(comparison_list_new[0]) >= 2)and len(comparison_list_new) == 1:
				for i in range(len(comparison_list_new[0])):
					for i in range(len(comparison_list_new[0])):
						index=k3.index(comparison_list_new[0][i])
						length=len(excel_df)
						urls.append(index)
					url=hrefs[index]
					for i in urls:
						final_urls.append(hrefs[i])
					final_urls = list(dict.fromkeys(final_urls))
				for i in range(len(comparison_list_new[0])):
					length=len(excel_df)	
					excel_df.loc[length+1] = ["-",city,comparison_list_new[0][i],final_urls[i]]
					print(excel_df)
			else:
				length=len(excel_df)
				excel_df.loc[length+1] = ["x",city,comparison_list_new[-1:][0],url]
			muÅŸ_check()
		

	else:
		label_negative_muÅŸ = tk.Label(lower_frame,text="MuÅŸ valiliÄŸinde 5 dakika sonra tekrardan arama yapÄ±lacak")
		label_negative_muÅŸ.pack()
		root.after(60000,label_negative_muÅŸ.destroy)
		root.after(60000,lambda:muÅŸ_web_scraping())
def ÅŸanlÄ±urfa_check(): 
    res = mb.askquestion('ÅžanlÄ±urfa Arama',  
                         'ÅžanlÄ±urfa ValiliÄŸinde Aramaya Devam Edilsin mi ?') 
      
    if res == 'yes' :
    	root.after(1000,lambda:ÅŸanlÄ±urfa_web_scraping())   
    else : 
        mb.showinfo('Return', 'Returning to the main application')
def ÅŸanlÄ±urfa_web_scraping():
	comparison_string= ["",""]
	comparison_list=[]
	a = []
	hrefs=[]
	arr= []
	page_arr = []
	urls= []
	final_urls = []
	word =get()
	url = "http://sanliurfa.gov.tr/duyurular"
	page = requests.get(url)
	data = page.text
	soup = BeautifulSoup(data, 'html.parser')
	text = soup.find_all('a',class_='announce-text')
	for line in soup.find_all('a',class_='announce-text'):
		a.append(line.get_text())
	for line in soup.find_all('a',class_='announce-text'):
		hrefs.append(line.get('href'))
	word=[y.lower() for y in word]
	k = [x.replace(' ', '') for x in a]
	k2= k = [x.replace('\r', '') for x in k]
	k3 = k = [x.replace('\n', '') for x in k2]
	k3=[x.lower() for x in k3]
	hrefs = list(dict.fromkeys(hrefs))

	for i in word:
		all_titles = [p for p in k3 if i in p]
		arr.append(all_titles)
	for i in comparison_string:
		all_titles = [p for p in k3 if i in p]
		comparison_list.append(all_titles)
	arr=tuple(filter(lambda x:x!=[], arr))
	mytuplelist = [tuple(item) for item in arr]
	mylist = list(set(mytuplelist))
	comparison_list_new = [elem for elem in arr if elem not in comparison_list ]
	# index=k3.index(comparison_list_new[0][0])
	num1= int(len(comparison_list_new))
	num2 = int(len(comparison_list_ÅŸanlÄ±urfa))
	ÅŸanlÄ±urfa_count = 0
	
	if num1 > num2:
		for i in range(len(comparison_list_new)):
			comparison_list_ÅŸanlÄ±urfa.append(comparison_list_new[i])
			index=k3.index(comparison_list_new[i][0])
			length=len(excel_df)
			urls.append(index)	
		url=hrefs[index]
		for i in urls:
			final_urls.append(hrefs[i])

		page = requests.get("http:"+url)
		data = page.text
		soup_text = BeautifulSoup(data, 'html.parser')
		for line in soup_text.find_all('div',class_='icerik'):
			page_arr.append(line.get_text())
		page_arr = [x.replace('\xa0', '') for x in page_arr]
		page_arr = [x.replace('\r', '') for x in page_arr]
		page_arr = [x.replace('\n', '') for x in page_arr]
		page_arr = [x.replace(' ', '') for x in page_arr]
		page_arr = [x.replace('/', '') for x in page_arr]
		page_arr=[x.lower() for x in page_arr]
		for i in page_arr:
			page_substring = [p for p in page_arr if ("2911" or "5944") in p]
			print(page_substring)
		if len(page_substring) != 0:
			messagebox.showinfo("ÅžanlÄ±urfa","Yeni Duyuru YayÄ±nlandÄ± !!!!")
			label_pos_ÅŸanlÄ±urfa = tk.Label(lower_frame,text="ÅžanlÄ±urfa ValiliÄŸinde yeni bildiri yayÄ±nlandÄ±")
			label_pos_ÅŸanlÄ±urfa.pack()
			root.after(60000,label_pos_ÅŸanlÄ±urfa.destroy)
			city = "ÅžanlÄ±urfa"
			if(len(comparison_list_new[0]) >= 2)and len(comparison_list_new) ==1:
				for i in range(len(comparison_list_new[0])):
					index=k3.index(comparison_list_new[0][i])
					length=len(excel_df)
					urls.append(index)
				url=hrefs[index]
				for i in urls:
					final_urls.append(hrefs[i])
				final_urls = list(dict.fromkeys(final_urls))
				print(final_urls)
				for i in range(len(comparison_list_new[0])):
					length=len(excel_df)	
					excel_df.loc[length+1] = ["+",city,comparison_list_new[0][i],final_urls[i]]
					print(excel_df)
			else:
				length=len(excel_df)
				excel_df.loc[length+1] = ["+",city,comparison_list_new[-1:][0],url]
			
			print(num1," ",num2)
			num2+=1
			print(num1," ",num2)
			ÅŸanlÄ±urfa_check()
		else:
			messagebox.showinfo("ÅžanlÄ±urfa","Yeni Duyuru YayÄ±nlandÄ± !!!!")
			label_pos_ÅŸanlÄ±urfa = tk.Label(lower_frame,text="ÅžanlÄ±urfa ValiliÄŸinde yeni bildiri yayÄ±nlandÄ±")
			city = "ÅžanlÄ±urfa"
			if(len(comparison_list_new[0]) >= 2)and len(comparison_list_new) == 1:
				for i in range(len(comparison_list_new[0])):
					for i in range(len(comparison_list_new[0])):
						index=k3.index(comparison_list_new[0][i])
						length=len(excel_df)
						urls.append(index)
					url=hrefs[index]
					for i in urls:
						final_urls.append(hrefs[i])
					final_urls = list(dict.fromkeys(final_urls))
				for i in range(len(comparison_list_new[0])):
					length=len(excel_df)	
					excel_df.loc[length+1] = ["-",city,comparison_list_new[0][i],final_urls[i]]
					print(excel_df)
			else:
				length=len(excel_df)
				excel_df.loc[length+1] = ["x",city,comparison_list_new[-1:][0],url]
			ÅŸanlÄ±urfa_check()
		

	else:
		label_negative_ÅŸanlÄ±urfa = tk.Label(lower_frame,text="ÅžanlÄ±urfa valiliÄŸinde 5 dakika sonra tekrardan arama yapÄ±lacak")
		label_negative_ÅŸanlÄ±urfa.pack()
		root.after(60000,label_negative_ÅŸanlÄ±urfa.destroy)
		root.after(60000,lambda:ÅŸanlÄ±urfa_web_scraping())
def ÅŸÄ±rnak_check(): 
    res = mb.askquestion('ÅžÄ±rnak Arama',  
                         'ÅžÄ±rnak ValiliÄŸinde Aramaya Devam Edilsin mi ?') 
      
    if res == 'yes' :
    	root.after(1000,lambda:ÅŸÄ±rnak_web_scraping())   
    else : 
        mb.showinfo('Return', 'Returning to the main application')
def ÅŸÄ±rnak_web_scraping():
	comparison_string= ["",""]
	comparison_list=[]
	a = []
	hrefs=[]
	arr= []
	page_arr = []
	urls= []
	final_urls = []
	word =get()
	url = "http://www.sirnak.gov.tr/basin-aciklamalari"
	page = requests.get(url)
	data = page.text
	soup = BeautifulSoup(data, 'html.parser')
	text = soup.find_all('a',class_='announce-text')
	for line in soup.find_all('a',class_='announce-text'):
		a.append(line.get_text())
	for line in soup.find_all('a',class_='announce-text'):
		hrefs.append(line.get('href'))
	word=[y.lower() for y in word]
	k = [x.replace(' ', '') for x in a]
	k2= k = [x.replace('\r', '') for x in k]
	k3 = k = [x.replace('\n', '') for x in k2]
	k3=[x.lower() for x in k3]
	hrefs = list(dict.fromkeys(hrefs))

	for i in word:
		all_titles = [p for p in k3 if i in p]
		arr.append(all_titles)
	for i in comparison_string:
		all_titles = [p for p in k3 if i in p]
		comparison_list.append(all_titles)
	arr=tuple(filter(lambda x:x!=[], arr))
	mytuplelist = [tuple(item) for item in arr]
	mylist = list(set(mytuplelist))
	comparison_list_new = [elem for elem in arr if elem not in comparison_list ]
	# index=k3.index(comparison_list_new[0][0])
	num1= int(len(comparison_list_new))
	num2 = int(len(comparison_list_ÅŸÄ±rnak))
	ÅŸÄ±rnak_count = 0
	
	if num1 > num2:
		for i in range(len(comparison_list_new)):
			comparison_list_ÅŸÄ±rnak.append(comparison_list_new[i])
			index=k3.index(comparison_list_new[i][0])
			length=len(excel_df)
			urls.append(index)	
		url=hrefs[index]
		for i in urls:
			final_urls.append(hrefs[i])

		page = requests.get("http:"+url)
		data = page.text
		soup_text = BeautifulSoup(data, 'html.parser')
		for line in soup_text.find_all('div',class_='icerik'):
			page_arr.append(line.get_text())
		page_arr = [x.replace('\xa0', '') for x in page_arr]
		page_arr = [x.replace('\r', '') for x in page_arr]
		page_arr = [x.replace('\n', '') for x in page_arr]
		page_arr = [x.replace(' ', '') for x in page_arr]
		page_arr = [x.replace('/', '') for x in page_arr]
		page_arr=[x.lower() for x in page_arr]
		for i in page_arr:
			page_substring = [p for p in page_arr if ("2911" or "5944") in p]
			print(page_substring)
		if len(page_substring) != 0:
			messagebox.showinfo("ÅžÄ±rnak","Yeni Duyuru YayÄ±nlandÄ± !!!!")
			label_pos_ÅŸÄ±rnak = tk.Label(lower_frame,text="ÅžÄ±rnak ValiliÄŸinde yeni bildiri yayÄ±nlandÄ±")
			label_pos_ÅŸÄ±rnak.pack()
			root.after(60000,label_pos_ÅŸÄ±rnak.destroy)
			city = "ÅžÄ±rnak"
			if(len(comparison_list_new[0]) >= 2)and len(comparison_list_new) ==1:
				for i in range(len(comparison_list_new[0])):
					index=k3.index(comparison_list_new[0][i])
					length=len(excel_df)
					urls.append(index)
				url=hrefs[index]
				for i in urls:
					final_urls.append(hrefs[i])
				final_urls = list(dict.fromkeys(final_urls))
				print(final_urls)
				for i in range(len(comparison_list_new[0])):
					length=len(excel_df)	
					excel_df.loc[length+1] = ["+",city,comparison_list_new[0][i],final_urls[i]]
					print(excel_df)
			else:
				length=len(excel_df)
				excel_df.loc[length+1] = ["+",city,comparison_list_new[-1:][0],url]
			
			print(num1," ",num2)
			num2+=1
			print(num1," ",num2)
			ÅŸÄ±rnak_check()
		else:
			messagebox.showinfo("ÅžÄ±rnak","Yeni Duyuru YayÄ±nlandÄ± !!!!")
			label_pos_ÅŸÄ±rnak = tk.Label(lower_frame,text="ÅžÄ±rnak ValiliÄŸinde yeni bildiri yayÄ±nlandÄ±")
			city = "ÅžÄ±rnak"
			if(len(comparison_list_new[0]) >= 2)and len(comparison_list_new) == 1:
				for i in range(len(comparison_list_new[0])):
					for i in range(len(comparison_list_new[0])):
						index=k3.index(comparison_list_new[0][i])
						length=len(excel_df)
						urls.append(index)
					url=hrefs[index]
					for i in urls:
						final_urls.append(hrefs[i])
					final_urls = list(dict.fromkeys(final_urls))
				for i in range(len(comparison_list_new[0])):
					length=len(excel_df)	
					excel_df.loc[length+1] = ["-",city,comparison_list_new[0][i],final_urls[i]]
					print(excel_df)
			else:
				length=len(excel_df)
				excel_df.loc[length+1] = ["x",city,comparison_list_new[-1:][0],url]
			ÅŸÄ±rnak_check()
		

	else:
		label_negative_ÅŸÄ±rnak = tk.Label(lower_frame,text="ÅžÄ±rnak valiliÄŸinde 5 dakika sonra tekrardan arama yapÄ±lacak")
		label_negative_ÅŸÄ±rnak.pack()
		root.after(60000,label_negative_ÅŸÄ±rnak.destroy)
		root.after(60000,lambda:ÅŸÄ±rnak_web_scraping())
def tunceli_check(): 
    res = mb.askquestion('Tunceli Arama',  
                         'Tunceli ValiliÄŸinde Aramaya Devam Edilsin mi ?') 
      
    if res == 'yes' :
    	root.after(1000,lambda:tunceli_web_scraping())   
    else : 
        mb.showinfo('Return', 'Returning to the main application')
def tunceli_web_scraping():
	comparison_string= ["",""]
	comparison_list=[]
	a = []
	hrefs=[]
	arr= []
	page_arr = []
	urls= []
	final_urls = []
	word =get()
	url = "http://tunceli.gov.tr/duyurular"
	page = requests.get(url)
	data = page.text
	soup = BeautifulSoup(data, 'html.parser')
	text = soup.find_all('a',class_='announce-text')
	for line in soup.find_all('a',class_='announce-text'):
		a.append(line.get_text())
	for line in soup.find_all('a',class_='announce-text'):
		hrefs.append(line.get('href'))
	word=[y.lower() for y in word]
	k = [x.replace(' ', '') for x in a]
	k2= k = [x.replace('\r', '') for x in k]
	k3 = k = [x.replace('\n', '') for x in k2]
	k3=[x.lower() for x in k3]
	hrefs = list(dict.fromkeys(hrefs))

	for i in word:
		all_titles = [p for p in k3 if i in p]
		arr.append(all_titles)
	for i in comparison_string:
		all_titles = [p for p in k3 if i in p]
		comparison_list.append(all_titles)
	arr=tuple(filter(lambda x:x!=[], arr))
	mytuplelist = [tuple(item) for item in arr]
	mylist = list(set(mytuplelist))
	comparison_list_new = [elem for elem in arr if elem not in comparison_list ]
	# index=k3.index(comparison_list_new[0][0])
	num1= int(len(comparison_list_new))
	num2 = int(len(comparison_list_tunceli))
	tunceli_count = 0
	
	if num1 > num2:
		for i in range(len(comparison_list_new)):
			comparison_list_tunceli.append(comparison_list_new[i])
			index=k3.index(comparison_list_new[i][0])
			length=len(excel_df)
			urls.append(index)	
		url=hrefs[index]
		for i in urls:
			final_urls.append(hrefs[i])

		page = requests.get("http:"+url)
		data = page.text
		soup_text = BeautifulSoup(data, 'html.parser')
		for line in soup_text.find_all('div',class_='icerik'):
			page_arr.append(line.get_text())
		page_arr = [x.replace('\xa0', '') for x in page_arr]
		page_arr = [x.replace('\r', '') for x in page_arr]
		page_arr = [x.replace('\n', '') for x in page_arr]
		page_arr = [x.replace(' ', '') for x in page_arr]
		page_arr = [x.replace('/', '') for x in page_arr]
		page_arr=[x.lower() for x in page_arr]
		for i in page_arr:
			page_substring = [p for p in page_arr if ("2911" or "5944") in p]
			print(page_substring)
		if len(page_substring) != 0:
			messagebox.showinfo("Tunceli","Yeni Duyuru YayÄ±nlandÄ± !!!!")
			label_pos_tunceli = tk.Label(lower_frame,text="Tunceli ValiliÄŸinde yeni bildiri yayÄ±nlandÄ±")
			label_pos_tunceli.pack()
			root.after(60000,label_pos_tunceli.destroy)
			city = "Tunceli"
			if(len(comparison_list_new[0]) >= 2)and len(comparison_list_new) ==1:
				for i in range(len(comparison_list_new[0])):
					index=k3.index(comparison_list_new[0][i])
					length=len(excel_df)
					urls.append(index)
				url=hrefs[index]
				for i in urls:
					final_urls.append(hrefs[i])
				final_urls = list(dict.fromkeys(final_urls))
				print(final_urls)
				for i in range(len(comparison_list_new[0])):
					length=len(excel_df)	
					excel_df.loc[length+1] = ["+",city,comparison_list_new[0][i],final_urls[i]]
					print(excel_df)
			else:
				length=len(excel_df)
				excel_df.loc[length+1] = ["+",city,comparison_list_new[-1:][0],url]
			
			print(num1," ",num2)
			num2+=1
			print(num1," ",num2)
			tunceli_check()
		else:
			messagebox.showinfo("Tunceli","Yeni Duyuru YayÄ±nlandÄ± !!!!")
			label_pos_tunceli = tk.Label(lower_frame,text="Tunceli ValiliÄŸinde yeni bildiri yayÄ±nlandÄ±")
			city = "Tunceli"
			if(len(comparison_list_new[0]) >= 2)and len(comparison_list_new) == 1:
				for i in range(len(comparison_list_new[0])):
					for i in range(len(comparison_list_new[0])):
						index=k3.index(comparison_list_new[0][i])
						length=len(excel_df)
						urls.append(index)
					url=hrefs[index]
					for i in urls:
						final_urls.append(hrefs[i])
					final_urls = list(dict.fromkeys(final_urls))
				for i in range(len(comparison_list_new[0])):
					length=len(excel_df)	
					excel_df.loc[length+1] = ["-",city,comparison_list_new[0][i],final_urls[i]]
					print(excel_df)
			else:
				length=len(excel_df)
				excel_df.loc[length+1] = ["x",city,comparison_list_new[-1:][0],url]
			tunceli_check()
		

	else:
		label_negative_tunceli = tk.Label(lower_frame,text="Tunceli valiliÄŸinde 5 dakika sonra tekrardan arama yapÄ±lacak")
		label_negative_tunceli.pack()
		root.after(60000,label_negative_tunceli.destroy)
		root.after(60000,lambda:tunceli_web_scraping())
def van_check(): 
    res = mb.askquestion('Van Arama',  
                         'Van ValiliÄŸinde Aramaya Devam Edilsin mi ?') 
      
    if res == 'yes' :
    	root.after(1000,lambda:van_web_scraping())   
    else : 
        mb.showinfo('Return', 'Returning to the main application')
def van_web_scraping():
	comparison_string= ["",""]
	comparison_list=[]
	a = []
	hrefs=[]
	arr= []
	page_arr = []
	urls= []
	final_urls = []
	word =get()
	url = "http://van.gov.tr/duyurular"
	page = requests.get(url)
	data = page.text
	soup = BeautifulSoup(data, 'html.parser')
	text = soup.find_all('a',class_='announce-text')
	for line in soup.find_all('a',class_='announce-text'):
		a.append(line.get_text())
	for line in soup.find_all('a',class_='announce-text'):
		hrefs.append(line.get('href'))
	word=[y.lower() for y in word]
	k = [x.replace(' ', '') for x in a]
	k2= k = [x.replace('\r', '') for x in k]
	k3 = k = [x.replace('\n', '') for x in k2]
	k3=[x.lower() for x in k3]
	hrefs = list(dict.fromkeys(hrefs))

	for i in word:
		all_titles = [p for p in k3 if i in p]
		arr.append(all_titles)
	for i in comparison_string:
		all_titles = [p for p in k3 if i in p]
		comparison_list.append(all_titles)
	arr=tuple(filter(lambda x:x!=[], arr))
	mytuplelist = [tuple(item) for item in arr]
	mylist = list(set(mytuplelist))
	comparison_list_new = [elem for elem in arr if elem not in comparison_list ]
	# index=k3.index(comparison_list_new[0][0])
	num1= int(len(comparison_list_new))
	num2 = int(len(comparison_list_van))
	van_count = 0
	
	if num1 > num2:
		for i in range(len(comparison_list_new)):
			comparison_list_van.append(comparison_list_new[i])
			index=k3.index(comparison_list_new[i][0])
			length=len(excel_df)
			urls.append(index)	
		url=hrefs[index]
		for i in urls:
			final_urls.append(hrefs[i])

		page = requests.get("http:"+url)
		data = page.text
		soup_text = BeautifulSoup(data, 'html.parser')
		for line in soup_text.find_all('div',class_='icerik'):
			page_arr.append(line.get_text())
		page_arr = [x.replace('\xa0', '') for x in page_arr]
		page_arr = [x.replace('\r', '') for x in page_arr]
		page_arr = [x.replace('\n', '') for x in page_arr]
		page_arr = [x.replace(' ', '') for x in page_arr]
		page_arr = [x.replace('/', '') for x in page_arr]
		page_arr=[x.lower() for x in page_arr]
		for i in page_arr:
			page_substring = [p for p in page_arr if ("2911" or "5944") in p]
			print(page_substring)
		if len(page_substring) != 0:
			messagebox.showinfo("Van","Yeni Duyuru YayÄ±nlandÄ± !!!!")
			label_pos_van = tk.Label(lower_frame,text="Van ValiliÄŸinde yeni bildiri yayÄ±nlandÄ±")
			label_pos_van.pack()
			root.after(60000,label_pos_van.destroy)
			city = "Van"
			if(len(comparison_list_new[0]) >= 2)and len(comparison_list_new) ==1:
				for i in range(len(comparison_list_new[0])):
					index=k3.index(comparison_list_new[0][i])
					length=len(excel_df)
					urls.append(index)
				url=hrefs[index]
				for i in urls:
					final_urls.append(hrefs[i])
				final_urls = list(dict.fromkeys(final_urls))
				print(final_urls)
				for i in range(len(comparison_list_new[0])):
					length=len(excel_df)	
					excel_df.loc[length+1] = ["+",city,comparison_list_new[0][i],final_urls[i]]
					print(excel_df)
			else:
				length=len(excel_df)
				excel_df.loc[length+1] = ["+",city,comparison_list_new[-1:][0],url]
			
			print(num1," ",num2)
			num2+=1
			print(num1," ",num2)
			van_check()
		else:
			messagebox.showinfo("Van","Yeni Duyuru YayÄ±nlandÄ± !!!!")
			label_pos_van = tk.Label(lower_frame,text="Van ValiliÄŸinde yeni bildiri yayÄ±nlandÄ±")
			city = "Van"
			if(len(comparison_list_new[0]) >= 2)and len(comparison_list_new) == 1:
				for i in range(len(comparison_list_new[0])):
					for i in range(len(comparison_list_new[0])):
						index=k3.index(comparison_list_new[0][i])
						length=len(excel_df)
						urls.append(index)
					url=hrefs[index]
					for i in urls:
						final_urls.append(hrefs[i])
					final_urls = list(dict.fromkeys(final_urls))
				for i in range(len(comparison_list_new[0])):
					length=len(excel_df)	
					excel_df.loc[length+1] = ["-",city,comparison_list_new[0][i],final_urls[i]]
					print(excel_df)
			else:
				length=len(excel_df)
				excel_df.loc[length+1] = ["x",city,comparison_list_new[-1:][0],url]
			van_check()
		

	else:
		label_negative_van = tk.Label(lower_frame,text="Van valiliÄŸinde 5 dakika sonra tekrardan arama yapÄ±lacak")
		label_negative_van.pack()
		root.after(60000,label_negative_van.destroy)
		root.after(60000,lambda:van_web_scraping())


def google():
	webbrowser.open("www.Google.com")
	

def helpmenu():
	os.system('start Valilik_Arama_KullanÄ±m_KÄ±lavuzu.docx')



def searching_displaying_websites():
	# for url in keywords:
	# 	webbrowser.open(url)
	if len(keywords) == 0:
		global label_send
		label_send = tk.Label(lower_frame,text="Bir Anahtar Kelime YazÄ±p YazdÄ±klarÄ±mÄ± ilete tÄ±klayÄ±p Tekrar Deneyiniz",font=30,bg="#ECECF2")
		label_send.pack()
	else:
		for term in keywords:
			url = "https://www.google.com.tr/search?q={}".format(keywords)
		webbrowser.open_new_tab(url)
		

def iterating_google_search():
	query = keywords
	if len(keywords) == 0:
		global label1
		label1= tk.Label(lower_frame,text="Aranacak Kelimeyi ekleyin.",font=60,bg="#ECECF2")
		label1.pack()
	else:
		for i in range(len(keywords)):
			for j in search(keywords[i], num=20, stop=10):
				websites.append(j)
		
		global scrollbar
		scrollbar=tk.Scrollbar(lower_frame,orient=tk.VERTICAL)
		
		scrollbar.pack(side=tk.RIGHT, fill= tk.Y)
		global T
		T=tk.Text(lower_frame,bg="#ECECF2",yscrollcommand = scrollbar.set)
		scrollbar.config(command=T.yview)
		T.pack()

		
		for display in websites:
			T.insert(tk.END,display+"\n")
			

def deleting_Screen():
	try:
		T
	except NameError:
		print("do nothing")
	else:
		T.destroy()
		scrollbar.destroy()
	try:
		label1
	except NameError:
		print("do nothing")
	else:
		label1.destroy()
	try:
		label
	except NameError:
		print("do nothing")
	else:
		label.destroy()
	try:
		label3
	except NameError:
		print("do nothing")
	else:
		label3.destroy()
	try:
		label_send
	except NameError:
		print("do nothing")
	else:
		label_send.destroy()
	try:
		label_excel
	except NameError:
		print("do nothing")
	else:
		label_excel.destroy()


def deleting_keywords():
	res.clear()
	print(res)

def saving_excel():
	export_file_path = filedialog.asksaveasfilename(defaultextension='.xlsx')
	excel_df.to_excel (export_file_path, index = False)
def displaying_excel():
	global label_excel
	label_excel=tk.Label(lower_frame,text=excel_df)
	label_excel.pack()

def start_search_all():
	adana_web_scraping()
	root.after(30000,lambda:batman_web_scraping())
	root.after(45000,lambda:elazÄ±ÄŸ_web_scraping())
	root.after(60000,lambda:gaziantep_web_scraping())
	root.after(75000,lambda:hakkari_web_scraping())
	root.after(90000,lambda:mardin_web_scraping())
	root.after(105000,lambda:muÄŸla_web_scraping())
	root.after(120000,lambda:muÅŸ_web_scraping())
	root.after(135000,lambda:ÅŸanlÄ±urfa_web_scraping())
	root.after(150000,lambda:ÅŸÄ±rnak_web_scraping())
	root.after(165000,lambda:tunceli_web_scraping())
	root.after(180000,lambda:van_web_scraping())
	root.after(195000,lambda:adana_press_scraping())
root = tk.Tk()
root.wm_iconbitmap('eshid.ico')
root.wm_title('Title')
root.geometry("700x500")
root.title("EÅžHÄ°D GOOGLE TARAMA")
root.configure(bg="#E5E6E2")
network_control()
img  = Image.open("eshidd.png") 
photo=ImageTk.PhotoImage(img)
lab=tk.Label(image=photo).place(x=0,y=0)
 
label = tk.Label(root,text="EÅŸhid Google Tarama",bg="#E5E6E2",font=40)
label.pack()



Frame=tk.Frame(root,bg="#4784BC",bd=5)
Frame.place(relx=0.5,rely=0.1,relwidth=0.75,relheight=0.132,anchor="n")

Entry=tk.Entry(Frame,font=40,bd=4)
Entry.place(relwidth=0.5,relheight=0.5,relx=0.01)



Keyentry=tk.Entry(Frame,font=40,bd=4)
Keyentry.place(relx=0.01,rely=0.82,anchor="w",relwidth=0.50,relheight=0.50)


buttonstart = tk.Button(Frame,text="Kelimeleri Google'da Ara",font=40,command =lambda:searching_displaying_websites())
buttonstart.place(relx = 0.60,rely=0.27,anchor='w',relwidth=0.38,relheight=0.50)



Buttonstop=tk.Button(Frame,text="YazdÄ±klarÄ±nÄ± Ä°let",font=40,command=lambda:get())
Buttonstop.place(relx=0.60,rely=0.85,anchor="w",relwidth=0.38,relheight=0.50)

lower_frame=tk.Entry(root,bg="#ECECF2",bd=5)
lower_frame.place(relx=0.5,rely=0.4,relwidth=0.8,relheight=0.5,anchor="n")


			

my_menu=tk.Menu(root)
root.config(menu=my_menu)

file_menu = tk.Menu(my_menu)
my_menu.add_cascade(label="MenÃ¼",menu=file_menu)
file_menu.add_command(label="Google'Ä± AÃ§",command=lambda:google())
file_menu.add_command(label="Google SonuÃ§ Tarama",command=lambda:iterating_google_search())
file_menu.add_command(label="Google'da Arama",command=lambda:searching_displaying_websites())
file_menu.add_separator()
file_menu.add_command(label="Ã‡Ä±kÄ±ÅŸ",command= root.quit)

################################################################################


website_menu=tk.Menu(my_menu)
my_menu.add_cascade(label="Duyurularda Arama",menu=website_menu)
website_menu.add_command(label="TÃ¼m Valiliklerde Arama",command=lambda:start_search_all())
website_menu.add_command(label="Adana",command=lambda:adana_web_scraping())
website_menu.add_command(label="Adana2",command=lambda:adana_press_scraping())
website_menu.add_command(label="Batman",command=lambda:batman_web_scraping())
website_menu.add_command(label="ElazÄ±ÄŸ",command=lambda:elazÄ±ÄŸ_web_scraping())
website_menu.add_command(label="Gaziantep",command=lambda:gaziantep_web_scraping())
website_menu.add_command(label="Hakkari",command=lambda:hakkari_web_scraping())
website_menu.add_command(label="Mardin",command=lambda:mardin_web_scraping())
website_menu.add_command(label="MuÄŸla",command=lambda:muÄŸla_web_scraping())
website_menu.add_command(label="MuÅŸ",command=lambda:muÅŸ_web_scraping())
website_menu.add_command(label="ÅžanlÄ±urfa",command=lambda:ÅŸanlÄ±urfa_web_scraping())
website_menu.add_command(label="ÅžÄ±rnak",command=lambda:ÅŸÄ±rnak_web_scraping())
website_menu.add_command(label="Tunceli",command=lambda:tunceli_web_scraping())
website_menu.add_command(label="Van",command=lambda:van_web_scraping())


################################################################################

edit_menu = tk.Menu(my_menu)
my_menu.add_cascade(label="Belgeler",menu=edit_menu)
edit_menu.add_command(label="Kaydedilen Valilikleri GÃ¶ster",command=lambda:displaying_excel())
edit_menu.add_command(label="Bulunan Siteleri Excel dosyasÄ±na aktar",command=lambda:saving_excel())

###############################################################################

delete_menu = tk.Menu(my_menu)
my_menu.add_cascade(label="YazÄ±lanlarÄ± Sil",menu=delete_menu)
delete_menu.add_command(label="Ekrandaki YazÄ±larÄ± Temizle",command=lambda:deleting_Screen())
delete_menu.add_command(label="GÃ¶nderilen YazÄ±larÄ± Temizle",command=lambda:deleting_keywords())

##############################################################################


help_menu = tk.Menu(my_menu)
my_menu.add_cascade(label="Help",menu=help_menu)
help_menu.add_command(label="Help",command=lambda:helpmenu())




# for j in search(query, tld="co.in", num=10, stop=10):
# 	websites.append(j) 
# print(websites)



root.mainloop()


